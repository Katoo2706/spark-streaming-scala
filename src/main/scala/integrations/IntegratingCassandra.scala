package integrations

import com.datastax.spark.connector.cql.CassandraConnector
import org.apache.spark.sql.{Dataset, ForeachWriter, SparkSession}
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.cassandra._
import common._

object IntegratingCassandra extends App {

  val spark = SparkSession.builder()
    .appName("Integrating Cassandra")
    .master("local[2]")
    .getOrCreate()

  spark.sparkContext.setLogLevel("WARN")

  import spark.implicits._

  /**
   * Streaming to Cassandra in Batch mode
   * */
  def writeStreamToCassandraInBatches() = {
    val carsDS = spark.readStream
      .schema(carsSchema)
      .json("src/main/resources/data/cars")
      .as[Cars]

    /**
     * Spark will assume that Cassandra server is running locally if we don't specify:
     * .option("spark.cassandra.connection.host", "your_cassandra_host")
     * .option("spark.cassandra.connection.port", "your_cassandra_port")
     * .option("spark.cassandra.auth.username", "your_cassandra_username")
     * .option("spark.cassandra.auth.password", "your_cassandra_password")
     * */
    carsDS.writeStream
      .foreachBatch {
        (batch: Dataset[Cars], _: Long) =>
          // save this batch to Cassandra in a single table write
          batch
            .select(col("Name"), col("Horsepower"))
            .write
            .cassandraFormat("cars", "public") // table name and keyspace
            .option("spark.cassandra.connection.host", "localhost")
            .option("spark.cassandra.connection.port", "9042")
            .mode("append")
            .save()
      }
      .start()
      .awaitTermination()
  }

  /** 2/
   * ForeachWriter: New advanced technique
   * The abstract class for writing custom logic to process data generated by a query. */
  class CarCassandraForeachWriter extends ForeachWriter[Cars] {
    /*
    * - On every batch, on every partition `partitionId`
    *   - On every "epoch" = chunk of data
    *     - call the open method; if false, skip this chunk
    *     - for each entry in this chunk, call the process method (session)
    *     - call the close method either at the end of the chunk or with an error if it was thrown
    * */

    val keyspace = "public"
    val table = "cars"
    val connector = CassandraConnector(spark.sparkContext.getConf)

    override def open(partitionId: Long, epochId: Long): Boolean = {
      println("Open connection")
      true // not skip this chunk
    }

    override def process(car: Cars): Unit = {
      connector.withSessionDo { session =>
        // car name is a tring
        session.execute(
          s"""
            |INSERT INTO $keyspace.$table("Name", "Horsepower")
            |VALUES ('${car.Name}', ${car.Horsepower.orNull})
            |""".stripMargin
        )
      }
    }

    override def close(errorOrNull: Throwable): Unit = println("Closing connection")

  }

  def writeStreamToCassandra(): Unit = {
    val carsDS = spark.readStream
      .schema(carsSchema)
      .json("src/main/resources/data/cars")
      .as[Cars]

    carsDS
      .writeStream
      .foreach(new CarCassandraForeachWriter)
      .start()
      .awaitTermination()
  }

  writeStreamToCassandra()
}
